from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Optional
import requests
import json
from prompts.system_prompt_tool_info import get_system_prompt_tool_info
from prompts.tool_info import TOOL_INFO
from prompts.conversational_responses import get_random_conversational_response
import importlib
import os
import logging
from utils.image_helpers import (
    perform_background_removal,
    validate_image_path,
    load_image_from_path,
    generate_filename_from_path,
    save_processed_image_png,
)
from cv_api import (
    FunRequest,
    api_image_background_removal,
    api_image_portrait_effect,
    api_image_super_resolution,
    api_image_classify
)
from data_models import ImageRequest
from thefuzz import process

router = APIRouter()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LLMRequest(BaseModel):
    command: str
    context: Optional[dict] = None


def build_response(
    success: bool,
    tool_name: str = None,
    error: str = None,
    text: str = None,
    params: dict = None,
    message: str = None,
):
    """Builds a consistent API response object."""
    if text is None and success:
        text = get_random_conversational_response()
    elif text is None and not success:
        text = "I'm sorry, I ran into a problem. " + (error or "Please try again.")

    return {
        "success": success,
        "tool_name": tool_name,
        "error": error,
        "text": text,
        "params": params or {},
        "message": message,
    }


def _get_image_uri_from_context(context: Optional[dict]) -> Optional[str]:
    """Extracts the image URI from the request context, prioritizing preview over timeline."""
    logger.info("Attempting to extract image URI from context...")
    if not context:
        logger.warning("Context is empty.")
        return None

    # Check preview context first
    preview_context = context.get("preview")
    if (
        preview_context
        and preview_context.get("selected")
        and isinstance(preview_context.get("selectedData"), dict)
        and preview_context["selectedData"].get("localpath")
    ):

        image_uri = preview_context["selectedData"]["localpath"]
        logger.info(f"Found image URI in 'preview' context: {image_uri}")
        return image_uri

    # Check timeline context if not in preview
    timeline_context = context.get("timeline")
    if (
        timeline_context
        and timeline_context.get("selected")
        and isinstance(timeline_context.get("selectedData"), dict)
        and timeline_context["selectedData"].get("localpath")
    ):

        image_uri = timeline_context["selectedData"]["localpath"]
        logger.info(f"Found image URI in 'timeline' context: {image_uri}")
        return image_uri

    logger.warning("No usable image path found in preview or timeline context.")
    return None


def call_llm(messages, temperature=0.0):
    """Helper function to make a call to the LLM."""
    headers = {
        "accept": "*/*",
        "Authorization": "Bearer F7AN14R-59X4B0X-J060Q8D-05W2ANW",
        "Content-Type": "application/json",
    }

    request_body = {
        "messages": messages,
        "model": "quartz",
        "stream": False,
        "temperature": temperature,
    }

    response = requests.post(
        "http://localhost:49728/api/v1/openai/chat/completions",
        json=request_body,
        headers=headers,
    )

    response.raise_for_status()  # Raise an exception for bad status codes
    return response.json()


@router.post("/api/llm")
async def getResponseFromLlama3(request: LLMRequest):
    try:
        print(json.dumps(request.context, indent=2))
        logger.info("ðŸ¤– Starting API call to getResponseFromLlama3")
        logger.info(f"Received command: '{request.command}'")

        # Step 1: Identify the tool
        logger.info("Step 1: Identifying tool from command...")
        system_prompt_tool_info = get_system_prompt_tool_info()
        messages_tool_identification = [
            {"role": "system", "content": system_prompt_tool_info},
            {"role": "user", "content": request.command},
        ]

        tool_identification_response = call_llm(messages_tool_identification)

        try:
            # The response content from the LLM is a JSON string.
            tool_data = json.loads(
                tool_identification_response["choices"][0]["message"]["content"]
            )
            llm_tool_name = tool_data.get("tool")

            # Find the best match for the tool name
            available_tools = list(TOOL_INFO.keys()) + ["NULL"]
            best_match, score = process.extractOne(llm_tool_name, available_tools)

            tool_name = llm_tool_name
            if score > 80:  # Confidence threshold
                tool_name = best_match

            logger.info(
                f"LLM identified tool: '{llm_tool_name}', Matched tool: '{tool_name}' with score {score}"
            )

        except (json.JSONDecodeError, KeyError, IndexError) as e:
            logger.error(f"Failed to parse tool identification response: {e}")
            return build_response(
                success=False,
                error=f"Could not parse tool identification response: {e}",
            )

        # Step 2: Handle the identified tool
        logger.info("Step 2: Handling identified tool...")
        if tool_name in ["image_bg_remove", "add_portrait_effect", "make_super_res"]:
            logger.info(f"Handling context-based image tool: '{tool_name}'")
            image_uri = _get_image_uri_from_context(request.context)

            if not image_uri:
                error_msg = f"To use {tool_name}, please select an item on the timeline or in the preview."
                return build_response(
                    success=False, tool_name=tool_name, error=error_msg, text=error_msg
                )

            if image_uri.startswith("file://"):
                image_uri = image_uri.replace("file://", "", 1)

            image_uri = os.path.normpath(image_uri)
            logger.info(f"Normalized image URI: {image_uri}")

            # Call the appropriate cv_api function
            api_request = ImageRequest(image_path=image_uri)
            api_response = None
            if tool_name == "image_bg_remove":
                logger.info("Calling cv_api for background removal...")
                api_response = await api_image_background_removal(api_request)
            elif tool_name == "add_portrait_effect":
                logger.info("Calling cv_api for portrait effect...")
                api_response = await api_image_portrait_effect(api_request)
            else:  # make_super_res
                logger.info("Calling cv_api for super resolution...")
                api_response = await api_image_super_resolution(api_request)

            logger.info("Received response from cv_api.")

            if api_response and api_response.get("success"):
                return build_response(
                    success=True,
                    tool_name=tool_name,
                    params={"imageUri": api_response["data"].link},
                )
            else:
                error_message = (
                    api_response.get("error")
                    if isinstance(api_response, dict)
                    else "Unknown error from CV API"
                )
                logger.error(f"cv_api call failed for '{tool_name}': {error_message}")
                return build_response(
                    success=False, tool_name=tool_name, error=error_message
                )

        elif tool_name in TOOL_INFO:
            logger.info(f"Handling tool '{tool_name}' with parameter extraction.")
            try:
                # Dynamically import the parameter extraction prompt for the identified tool
                logger.info(f"Loading parameter extraction prompt for '{tool_name}'...")
                param_extraction_module = importlib.import_module(
                    f"prompts.{tool_name}.system_prompt_param_extraction"
                )
                get_system_prompt_for_param_extraction = getattr(
                    param_extraction_module, "get_system_prompt_for_param_extraction"
                )
                logger.info("Prompt loaded successfully.")
            except (ImportError, AttributeError) as e:
                error_msg = f"Could not load parameter extraction prompt for tool '{tool_name}': {e}"
                logger.error(
                    f"Failed to load parameter extraction prompt for '{tool_name}': {e}"
                )
                return build_response(success=False, error=error_msg)

            # Create user content for parameter extraction
            user_content = f"Command: {request.command}"
            if request.context:
                user_content += f"\nContext: {json.dumps(request.context)}"

            # Call LLM again to extract parameters
            logger.info("Calling LLM to extract parameters...")
            system_prompt_param_extraction = get_system_prompt_for_param_extraction(
                tool_name
            )
            messages_param_extraction = [
                {"role": "system", "content": system_prompt_param_extraction},
                {"role": "user", "content": user_content},
            ]

            param_extraction_response = call_llm(
                messages_param_extraction, temperature=0.1
            )
            logger.info("Received parameter extraction response from LLM.")

            try:
                extracted_params = json.loads(
                    param_extraction_response["choices"][0]["message"]["content"]
                )
                logger.info(f"Extracted parameters: {extracted_params}")

                if tool_name == "add_file":
                    file_to_add = extracted_params.get("fileName")
                    if not file_to_add or file_to_add == "NULL":
                        error_msg = (
                            "Could not identify the file to add from your command."
                        )
                        logger.error(
                            "LLM did not extract a filename for add_file tool."
                        )
                        return build_response(
                            success=False, tool_name=tool_name, error=error_msg
                        )

                    files_in_context = request.context.get("files", [])
                    if not files_in_context:
                        error_msg = "There are no files available in the current context to add."
                        logger.error("No files found in context for add_file tool.")
                        return build_response(
                            success=False, tool_name=tool_name, error=error_msg
                        )

                    # Fuzzy search
                    best_match, score = process.extractOne(
                        file_to_add, files_in_context
                    )

                    if score > 80:  # Confidence threshold
                        logger.info(
                            f"Fuzzy search matched '{file_to_add}' to '{best_match}' with score {score}."
                        )
                        return build_response(
                            success=True,
                            tool_name=tool_name,
                            params={
                                "fileName": best_match,
                                "directory": request.context.get("current_directory"),
                            },
                            message=f"Successfully identified file '{best_match}'.",
                        )
                    else:
                        error_msg = f"I could not find a file named '{file_to_add}' in the available files."
                        logger.warning(
                            f"Could not find a confident match for file '{file_to_add}'. Best match '{best_match}' with score {score}."
                        )
                        return build_response(
                            success=False,
                            tool_name=tool_name,
                            params={"fileName": "NULL"},
                            error=error_msg,
                        )

                elif tool_name in ["add_text", "add_shape", "add_slide"]:
                    logger.info(f"Successfully processed '{tool_name}'.")
                    return build_response(
                        success=True, tool_name=tool_name, params=extracted_params
                    )

            except (json.JSONDecodeError, KeyError, IndexError) as e:
                error_msg = f"Could not parse parameter extraction response: {e}"
                logger.error(error_msg)
                return build_response(
                    success=False, tool_name=tool_name, error=error_msg
                )

        elif tool_name == "NULL":
            logger.info("No specific tool identified. Returning NULL response.")
            return build_response(
                success=True,
                tool_name="NULL",
                message="No specific tool identified for the command.",
            )

        else:
            # Handle unknown or not-yet-implemented tools
            message = f"Tool '{tool_name}' is identified but not implemented."
            logger.warning(message)
            return build_response(success=True, tool_name=tool_name, message=message)

    except requests.RequestException as e:
        error_msg = f"Error communicating with LLM service: {str(e)}"
        logger.critical(f"LLM service communication error: {e}")
        return build_response(success=False, error=error_msg)
    except HTTPException as e:
        logger.error(f"HTTP exception caught: {e.detail}")
        return build_response(success=False, error=e.detail)
    except Exception as e:
        logger.critical(f"An unexpected error occurred: {e}", exc_info=True)
        return build_response(success=False, error=str(e))
